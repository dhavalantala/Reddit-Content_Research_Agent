{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd167fe7",
   "metadata": {},
   "source": [
    "We scrap the reddit post with Python and Selenium. But it become incredibly hard to do for a number of reasons. \n",
    "1. Reddit doesn't rellt want everybody scraping their website even thougn it's public data. \n",
    "2. Evn if you were scraping it manually then you would have to parse all of that data. You'd have to understand the structured of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f945cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup\n",
    "\n",
    "setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee582d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24df861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit.models import RedditPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc7496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIGHT_DATA_REDDIT_SCRAPER_API_KEY = os.environ.get(\"BRIGHT_DATA_REDDIT_SCRAPER_API_KEY\")\n",
    "assert BRIGHT_DATA_REDDIT_SCRAPER_API_KEY is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70f2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crawl_headers():\n",
    "    return {\n",
    "\t\"Authorization\": f\"Bearer {BRIGHT_DATA_REDDIT_SCRAPER_API_KEY}\",\n",
    "\t\"Content-Type\": \"application/json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee53dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'snapshot_id': 's_mh33qu4v1vmcemmiz6'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 152be2ab870e1b2b12721b2aceb1245515f31ae9cfc54979faa9653ee16d3eaa\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "data = json.dumps({\n",
    "    \"input\": [{\"url\":\"https://www.reddit.com/r/battlefield2042/comments/1cmqs1d/official_update_on_the_next_battlefield_game/\"},{\"url\":\"https://www.reddit.com/r/singularity/comments/1cmoa52/former_google_ceo_on_ai_its_underhyped/\"},{\"url\":\"https://reddit.com/r/datascience/comments/1cmnf0m/technical_interview_python_sql_problem_but_not/\"},{\"url\":\"https://www.reddit.com/t/anime/\"}],\n",
    "})\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://api.brightdata.com/datasets/v3/trigger?dataset_id=gd_lvz8ah06191smkebj4&notify=false&include_errors=true\",\n",
    "    headers=headers,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d77c2423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s_mh33quie2cu5gp6u7c'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def perform_scrape_snapshot(subreddit_url, num_of_posts: int = 20):\n",
    "    url = \"https://api.brightdata.com/datasets/v3/trigger\"\n",
    "    headers = get_crawl_headers()\n",
    "    params = {\n",
    "    \t\"dataset_id\": \"gd_lvz8ah06191smkebj4\",\n",
    "    \t\"include_errors\": \"true\",\n",
    "    \t\"type\": \"discover_new\",\n",
    "    \t\"discover_by\": \"subreddit_url\",\n",
    "    \t\"limit_per_input\": \"100\",\n",
    "    }\n",
    "    data = [\n",
    "    \t{\"url\": f\"{subreddit_url}\",\"sort_by\":\"Top\",\"sort_by_time\":\"Today\",\"num_of_posts\":num_of_posts},\n",
    "    ]\n",
    "    \n",
    "    response = requests.post(url, headers=headers, params=params, json=data)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data.get(\"snapshot_id\")\n",
    "\n",
    "\n",
    "perform_scrape_snapshot(\"https://www.reddit.com/r/Django\", num_of_posts=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c61df2",
   "metadata": {},
   "source": [
    "\"sd_mh26rkjnjg0u6fkq3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7950bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_id = \"sd_mh26rkjnjg0u6fkq3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e3b8c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_snapshot_progress(snapshot_id: str) -> bool:\n",
    "    url = f\"https://api.brightdata.com/datasets/v3/progress/{snapshot_id}\"\n",
    "    headers = get_crawl_headers()\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data.get('status') == 'ready'\n",
    "\n",
    "\n",
    "get_snapshot_progress(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f8f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_snapshot(snapshot_id: str) -> dict:\n",
    "    url = f\"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}\"\n",
    "    headers = get_crawl_headers()\n",
    "    params = {\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "reddit_results = download_snapshot(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d02dd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea56a79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'post_id',\n",
       " 'url',\n",
       " 'title',\n",
       " 'description',\n",
       " 'comments',\n",
       " 'related_posts',\n",
       " 'community_name',\n",
       " 'num_upvotes',\n",
       " 'num_comments',\n",
       " 'date_posted']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_field_names = [field.name for field in RedditPost._meta.get_fields()]\n",
    "model_field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bc19ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'date_posted']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_fields = ['id', 'post_id',\n",
    " 'url',\n",
    " 'description',\n",
    " 'comments',\n",
    " 'related_posts',\n",
    " 'community_name',\n",
    " 'num_upvotes',\n",
    " 'num_comments']\n",
    "\n",
    "valid_fields = [x for x in model_field_names if x not in skip_fields]\n",
    "valid_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97ca3135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_1ocsnku https://www.reddit.com/r/django/comments/1ocsnku/should_i_really_use_celery_for_file_validation_in/ {'title': 'Should I really use Celery for file validation in Django or just keep it synchronous?', 'date_posted': '2025-10-21T23:34:12.057Z'}\n",
      "t3_1od31ew https://www.reddit.com/r/django/comments/1od31ew/migration_anxiety/ {'title': 'Migration anxiety', 'date_posted': '2025-10-22T08:54:01.854Z'}\n",
      "t3_1od6896 https://www.reddit.com/r/django/comments/1od6896/reusing_graphql_queries_within_django/ {'title': 'Reusing GraphQL Queries within Django', 'date_posted': '2025-10-22T11:58:05.129Z'}\n"
     ]
    }
   ],
   "source": [
    "for thread in reddit_results:\n",
    "    post_id = thread.get(\"post_id\")\n",
    "    url = thread.get(\"url\")\n",
    "    update_data = {k:v for k, v in thread.items() if k in valid_fields}\n",
    "    print(post_id, url, update_data)\n",
    "    RedditPost.objects.update_or_create(\n",
    "        post_id=post_id,\n",
    "        url=url,\n",
    "        defaults=update_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc7ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
